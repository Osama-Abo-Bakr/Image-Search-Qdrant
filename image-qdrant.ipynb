{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Image Search Using Pinecone and ConvBase For Feature Extraction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Main Library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "# import qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, Batch, Filter, MatchValue, FieldCondition \n",
    "\n",
    "\n",
    "# model\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Prepare Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data-sample\\00357563a7.jpg</td>\n",
       "      <td>3054</td>\n",
       "      <td>class-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data-sample\\003bd60fa9.jpg</td>\n",
       "      <td>3055</td>\n",
       "      <td>class-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data-sample\\01c6b7230c.jpg</td>\n",
       "      <td>3056</td>\n",
       "      <td>class-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data-sample\\024a037366.jpg</td>\n",
       "      <td>3057</td>\n",
       "      <td>class-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data-sample\\029c926ce9.jpg</td>\n",
       "      <td>3058</td>\n",
       "      <td>class-a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         path    id    class\n",
       "0  data-sample\\00357563a7.jpg  3054  class-a\n",
       "1  data-sample\\003bd60fa9.jpg  3055  class-b\n",
       "2  data-sample\\01c6b7230c.jpg  3056  class-a\n",
       "3  data-sample\\024a037366.jpg  3057  class-b\n",
       "4  data-sample\\029c926ce9.jpg  3058  class-a"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathes = [os.path.join('data-sample', image) for image in os.listdir('data-sample')]\n",
    "df = pd.DataFrame({'path': pathes,\n",
    "                   'id': np.arange(3054, 3054 + len(pathes), 1),\n",
    "                   'class': ['class-a', 'class-b'] * int(len(pathes)/2)}\n",
    "                  )\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Load Envirnoment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(override=True)\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY')\n",
    "QDRANT_URL = os.getenv('QDRANT_URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Model: VGG19 for Feature Extraction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): ConvMlp(\n",
       "    (fc1): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (act1): ReLU(inplace=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (act2): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model('vgg19', pretrained=True)\n",
    "model = nn.Sequential(*list(model.children())[:-1])\n",
    "_ = model.eval()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Extract Image Feature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_feature(image_paths: list):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    batch_fearures = []\n",
    "    for image_path in image_paths:\n",
    "        # Convert Image Path to Pillow\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = transform(image).unsqueeze(0)\n",
    "        \n",
    "        # Pass The Image to Model to Extract Feature.\n",
    "        with torch.no_grad():\n",
    "            conv_feature = model(image)\n",
    "            \n",
    "            image_features = conv_feature.view(conv_feature.size(0), -1).tolist()[0]\n",
    "        \n",
    "        # append Feature\n",
    "        batch_fearures.append(image_features)\n",
    "    \n",
    "    return batch_fearures\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectore Feature Length: 4096\n"
     ]
    }
   ],
   "source": [
    "results = extract_image_feature(image_paths=['data-sample/0a73823599.jpg', 'data-sample/866a4779a7.jpg'])\n",
    "\n",
    "vect_length = len(results[0])\n",
    "print(f'Vectore Feature Length: {vect_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Upserting to Pinecone`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Image-Search Already Exist.\n"
     ]
    }
   ],
   "source": [
    "# connect to Qdrant\n",
    "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "\n",
    "# Collecting configration\n",
    "collection_config = VectorParams(\n",
    "    size=vect_length,\n",
    "    distance=Distance.COSINE, \n",
    "    on_disk=True\n",
    ")\n",
    "\n",
    "\n",
    "## Create a collection\n",
    "try:\n",
    "    collec_name = 'Image-Search'\n",
    "    client.create_collection(collection_name=collec_name, vectors_config=collection_config)\n",
    "    print('Collection Created Successfuly')\n",
    "except:\n",
    "    print(f'Collection {collec_name} Already Exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data-sample\\00357563a7.jpg</td>\n",
       "      <td>3054</td>\n",
       "      <td>class-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data-sample\\003bd60fa9.jpg</td>\n",
       "      <td>3055</td>\n",
       "      <td>class-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data-sample\\01c6b7230c.jpg</td>\n",
       "      <td>3056</td>\n",
       "      <td>class-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data-sample\\024a037366.jpg</td>\n",
       "      <td>3057</td>\n",
       "      <td>class-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data-sample\\029c926ce9.jpg</td>\n",
       "      <td>3058</td>\n",
       "      <td>class-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>data-sample\\9d21019336.jpg</td>\n",
       "      <td>3289</td>\n",
       "      <td>class-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>data-sample\\9e020b77ac.jpg</td>\n",
       "      <td>3290</td>\n",
       "      <td>class-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>data-sample\\9efd18dd6c.jpg</td>\n",
       "      <td>3291</td>\n",
       "      <td>class-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>data-sample\\9f5fc65189.jpg</td>\n",
       "      <td>3292</td>\n",
       "      <td>class-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>data-sample\\9fd544a838.jpg</td>\n",
       "      <td>3293</td>\n",
       "      <td>class-b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           path    id    class\n",
       "0    data-sample\\00357563a7.jpg  3054  class-a\n",
       "1    data-sample\\003bd60fa9.jpg  3055  class-b\n",
       "2    data-sample\\01c6b7230c.jpg  3056  class-a\n",
       "3    data-sample\\024a037366.jpg  3057  class-b\n",
       "4    data-sample\\029c926ce9.jpg  3058  class-a\n",
       "..                          ...   ...      ...\n",
       "235  data-sample\\9d21019336.jpg  3289  class-b\n",
       "236  data-sample\\9e020b77ac.jpg  3290  class-a\n",
       "237  data-sample\\9efd18dd6c.jpg  3291  class-b\n",
       "238  data-sample\\9f5fc65189.jpg  3292  class-a\n",
       "239  data-sample\\9fd544a838.jpg  3293  class-b\n",
       "\n",
       "[240 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:16<01:55, 16.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in upserting: The write operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:01<00:00, 15.21s/it]\n"
     ]
    }
   ],
   "source": [
    "## Function for upserting data to Qdrant\n",
    "def upsert_to_qdrant(df, batch_size=32):\n",
    "\n",
    "    ## A list for failed_ids\n",
    "    failed_ids = []\n",
    "\n",
    "    for batch_start in tqdm(range(0, len(df), batch_size)):\n",
    "\n",
    "        try:\n",
    "            ## Prepare batches\n",
    "            batch_end = min(batch_start+batch_size, len(df))\n",
    "            pathes_batch = df['path'][batch_start: batch_end].tolist()\n",
    "            ids_batch = df['id'][batch_start: batch_end].tolist()     ## No need to be converted to string (Qdrant need integer)\n",
    "            \n",
    "            ## Payload\n",
    "            payload_batch = [{'class': cls} for cls in df['class'][batch_start: batch_end].tolist()]\n",
    "\n",
    "            ## Get Embeddings using HuggingFace model\n",
    "            embeds_batch = extract_image_feature(image_paths=pathes_batch)\n",
    "\n",
    "            ## Prepare to Qdrant\n",
    "            to_upsert = Batch(ids=ids_batch, vectors=embeds_batch, payloads=payload_batch)\n",
    "\n",
    "            ## Upsert to Qdrant\n",
    "            _ = client.upsert(collection_name=collec_name, wait=True, points=to_upsert)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error in upserting: {e}')\n",
    "            failed_ids.append(ids_batch)\n",
    "\n",
    "    return failed_ids\n",
    "\n",
    "\n",
    "## Apply the function\n",
    "failed_ids = upsert_to_qdrant(df=df, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status is: green\n",
      "Vectors Count is: 208\n"
     ]
    }
   ],
   "source": [
    "## Check Status of Collection after upserting\n",
    "collection_status = client.get_collection(collection_name=collec_name).status\n",
    "collection_count_vectors = client.get_collection(collection_name=collec_name).points_count\n",
    "\n",
    "print(f'Status is: {collection_status}')\n",
    "print(f'Vectors Count is: {collection_count_vectors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id=3160, version=2, score=0.77637136, payload={'class': 'class-a'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=3224, version=4, score=0.76000416, payload={'class': 'class-a'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=3164, version=2, score=0.72962016, payload={'class': 'class-a'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=3130, version=1, score=0.6861493, payload={'class': 'class-a'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=3220, version=4, score=0.6848402, payload={'class': 'class-a'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=3246, version=5, score=0.681923, payload={'class': 'class-a'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=3222, version=4, score=0.6789307, payload={'class': 'class-a'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=3166, version=2, score=0.6692039, payload={'class': 'class-a'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=3138, version=1, score=0.65337753, payload={'class': 'class-a'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=3148, version=1, score=0.6486234, payload={'class': 'class-a'}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_new_path = df['path'].iloc[-1]\n",
    "image_feats_new = extract_image_feature(image_paths=[image_new_path])[0]\n",
    "\n",
    "client.search(collection_name=collec_name, query_vector=image_feats_new, limit=10, \n",
    "              score_threshold=0.4, \n",
    "              query_filter=Filter(must=[FieldCondition(key='class', match=MatchValue(value='class-a'))]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectorDB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
